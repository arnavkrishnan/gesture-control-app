# âœ‹ Gesture-Controlled Web Interface

A hands-free Flask web app that uses your webcam to recognize real-time hand gestures â€” enabling you to scroll, click, and control the browser without a mouse or trackpad! ğŸ§ ğŸ“¸

## ğŸ”¥ Features

- ğŸ–ï¸ **Hand Gesture Recognition** powered by [MediaPipe](https://mediapipe.dev/)
- ğŸŒ **Live Webcam Feed** on the frontend for real-time feedback
- ğŸ§  **Gesture Classification Engine** that interprets hand landmarks
- ğŸ–±ï¸ **Built-in Actions**:
  - âœŒï¸ **Peace sign** â†’ Scroll up
  - ğŸ‘‡ **Inverted peace sign** â†’ Scroll down
  - âœ‹ **Open hand** â†’ Move cursor
  - â˜ï¸ **Index finger only** â†’ Click
  - âœŠ **Fist** â†’ Exit app

## ğŸ“¸ Live Webcam Feed

The webpage shows a real-time video stream from your webcam with gesture overlays, so you can see what the model is detecting and how it maps gestures to actions.

---

## ğŸš€ Getting Started

### 0. Prerequisites

- **Python 3.9+**
- **Conda** (recommended) â€” [Miniconda Installation Guide](https://www.anaconda.com/docs/getting-started/miniconda/install)
- **pip** installed (`conda install pip` if youâ€™re missing it)
- A webcam ğŸ“·

---

### 1. Clone the Repo

```bash
git clone https://github.com/arnavkrishnan/gesture-control-app.git
cd gesture-control-app
```

### 2. Allow Shell Script Execution
```bash
chmod +x run.sh
```

### 3. Run the App

```bash
./run.sh
```
###### This script will:
- âœ… Create (or activate) a Conda environment
- âœ… Install required Python packages
- âœ… Launch the Flask app and open the browser
- âœ… Start the gesture detection engine
###### Then youâ€™re good to go!


## ğŸ’¡ How It Works
Each hand gesture is converted into a binary list of five values representing whether each finger is extended:

| Gesture Name         | Finger Pattern     | Action        |
|----------------------|--------------------|---------------|
| Open Hand            | [1,1,1,1,1]         | Move cursor   |
| Peace Sign           | [0,1,1,0,0]         | Scroll up     |
| Inverted Peace Sign  | [0,0,0,1,1]         | Scroll down   |
| Index Finger Only    | [0,1,0,0,0]         | Click         |
| Fist                 | [0,0,0,0,0]         | Exit app      |

Gestures are recognized via MediaPipeâ€™s hand landmark tracking and classified with custom logic, then sent to the frontend using WebSockets for interaction.

## Future Ideas
- Drag and drop with gestures ğŸ–±ï¸
- Multi-gesture combos (e.g., swipe + click)
- Browser integration using Selenium or Puppeteer
- Dark mode toggle via palm rotation ğŸŒ—
- Visual gesture timeline or heatmap
- AI model training for personalized gestures

## ğŸ™Œ Want to Contribute?
#### Pull Requests are welcome. You can contact me on GitHub or via [email](mailto:arnav.s.krishnan@gmail.com).

***

# Thanks!
